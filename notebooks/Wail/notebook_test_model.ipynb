{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 11:21:47.485430: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-26 11:21:47.525406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 11:21:48.394496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from signlens.preprocessing.data import *\n",
    "from signlens.preprocessing.preprocess import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from signlens.model.model import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from signlens.params import *\n",
    "from utils.model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(random_state=None):\n",
    "    print(Fore.MAGENTA + Style.BRIGHT + \"\\n⭐️ Use case: preprocess\" + Style.RESET_ALL)\n",
    "\n",
    "    # Data loading\n",
    "    train = load_data_subset_csv(balanced=True, random_state=random_state)\n",
    "\n",
    "    # Train test split\n",
    "    X_files = train.file_path\n",
    "    y = encode_labels(train.sign)\n",
    "    X_train_files, X_val_files, y_train, y_val = train_test_split(X_files, y, test_size=0.2, stratify=y, random_state=random_state)\n",
    "\n",
    "    # Preprocessing\n",
    "    print(Fore.BLUE + f\"\\nPreprocessing {len(X_train_files)} training files...\" + Style.RESET_ALL)\n",
    "    X_train = preprocess_and_pad_sequences_from_pq_list(X_train_files)\n",
    "    print(Fore.BLUE + f\"\\nPreprocessing {len(X_val_files)} validation files...\" + Style.RESET_ALL)\n",
    "    X_val = preprocess_and_pad_sequences_from_pq_list(X_val_files)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train,epochs=EPOCHS, patience=20, verbose=1, batch_size=32, validation_data=None, shuffle=True):\n",
    "\n",
    "    print(Fore.MAGENTA + Style.BRIGHT + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "\n",
    "    new_model_required = ''\n",
    "    while new_model_required.lower() not in ['y', 'n']:\n",
    "        new_model_required = input(\"Do you want to train a new model from scratch? (y/n): \")\n",
    "\n",
    "    if new_model_required.strip().lower() == 'y':\n",
    "        paths = create_model_folder()\n",
    "        model = initialize_model(num_classes=NUM_CLASSES)\n",
    "    else:\n",
    "        model_base_dir_pattern = input(\"Enter the name (or a part of the name) of the model you want to load: \").strip()\n",
    "        model, paths = load_model(mode='most_recent', model_base_dir_pattern=model_base_dir_pattern, return_paths=True)\n",
    "\n",
    "    model = compile_model(model)\n",
    "    model, history = train_model(model, X_train, y_train,\n",
    "                                 patience=patience,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=verbose,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=validation_data,\n",
    "                                 model_save_epoch_path=paths['iter'],\n",
    "                                 shuffle=shuffle\n",
    "                                 )\n",
    "\n",
    "    val_accuracy = np.max(history.history['val_accuracy'])\n",
    "\n",
    "    params = dict(\n",
    "        context=\"train\",\n",
    "        training_frac=DATA_FRAC,\n",
    "        row_count=len(X_train),\n",
    "        num_classes=NUM_CLASSES,\n",
    "    )\n",
    "\n",
    "    save_results(params=params,\n",
    "                 metrics=dict(val_accuracy=val_accuracy),\n",
    "                 params_path=paths['params'],\n",
    "                 metrics_path=paths['metrics'],\n",
    "                 mode='train'\n",
    "                 )\n",
    "\n",
    "    save_model(model=model, model_path=paths['model'])\n",
    "\n",
    "    return model, paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "unique_train_test_split()\n",
    "X_train, X_val, y_train, y_val = preprocess(random_state=random_state)\n",
    "shuffle = (random_state is None) # shuffle in fit if random_state is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, paths = train(X_train, y_train, validation_data=(X_val, y_val), shuffle=shuffle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
